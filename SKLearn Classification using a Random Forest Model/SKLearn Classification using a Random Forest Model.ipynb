{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating system version.... Darwin-18.2.0-x86_64-i386-64bit\n",
      "Python version is........... 3.6.5\n",
      "scikit-learn version is..... 0.20.0\n",
      "pandas version is........... 0.23.4\n",
      "numpy version is.conda i........... 1.15.3\n",
      "matplotlib version is....... 3.0.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, SCORERS\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.externals import joblib\n",
    "from operator import itemgetter\n",
    "\n",
    "from tabulate import tabulate\n",
    "print('Operating system version....', platform.platform())\n",
    "print(\"Python version is........... %s.%s.%s\" % sys.version_info[:3])\n",
    "print('scikit-learn version is.....', sklearn.__version__)\n",
    "print('pandas version is...........', pd.__version__)\n",
    "print('numpy version is.conda i...........', np.__version__)\n",
    "print('matplotlib version is.......', matplotlib.__version__)\n",
    "\n",
    "class Timer:\n",
    "  def __init__(self):\n",
    "    self.start = time.time()\n",
    "\n",
    "  def restart(self):\n",
    "    self.start = time.time()\n",
    "\n",
    "  def get_time(self):\n",
    "    end = time.time()\n",
    "    m, s = divmod(end - self.start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str\n",
    "\n",
    "def Correlation_plot(df):\n",
    "    plt.ioff()\n",
    "    red_green = [\"#ff0000\", \"#00ff00\"]\n",
    "    sns.set_palette(red_green)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    g = sns.pairplot(df,\n",
    "                    diag_kind = 'kde',\n",
    "                    hue = 'click',\n",
    "                    markers = [\"o\", \"D\"],\n",
    "                    size = 1.5,\n",
    "                    aspect = 1,\n",
    "                    plot_kws = {\"s\": 6})\n",
    "    g.fig.subplots_adjust(right = 0.9)\n",
    "    plt.show()\n",
    "\n",
    "def LoadData():\n",
    "    global X_train, y_train, X_test, y_test, train, test \n",
    "    global feature_columns, response_column, n_features\n",
    "    \n",
    "    model_full = pd.read_csv(\"./newsletter_sent.csv\",sep=';')\n",
    "    \n",
    "#    id;click;email;gender;age;click_rate;geo;device\n",
    "\n",
    "    response_column = ['click']\n",
    "    feature_columns = ['id','email', 'gender', 'age', 'click_rate','geo','device']\n",
    "    n_features = len(feature_columns)\n",
    "    mask = feature_columns + response_column\n",
    "    \n",
    "    model = model_full[mask]\n",
    "    print('Model dataset:\\n', model.head(5))\n",
    "    print('\\nDescription of model dataset:\\n', model[feature_columns].describe(include='all'))\n",
    "    \n",
    "    Correlation_plot(model)\n",
    "\n",
    "    # Split the data into Train and Test with Train having 80% and test 20% each\n",
    "    train_full, test_full = np.split(model_full.sample(frac=1), [int(.8*len(model))])\n",
    "\n",
    "    X_train  = train_full[feature_columns].as_matrix()\n",
    "    y_train_ = train_full[response_column].as_matrix()\n",
    "    X_test   = test_full[feature_columns].as_matrix()\n",
    "    y_test_  = test_full[response_column].as_matrix()\n",
    "    \n",
    "    y_train = np.reshape(y_train_, len(y_train_))\n",
    "    y_test  = np.reshape(y_test_,  len(y_test_))\n",
    "    \n",
    "    train = train_full[mask]\n",
    "    test  = test_full[mask]\n",
    "    print('Shape of train: ', train.shape)\n",
    "    print('Shape of test:  ', test.shape)\n",
    "    return\n",
    "\n",
    "def ROC_Curve(rf, auc):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    rf_fit = rf.fit(X_train, y_train)\n",
    "    fit = one_hot_encoder.fit(rf.apply(X_train))\n",
    "    y_predicted = rf.predict_proba(X_test)[:, 1]\n",
    "    false_positive, true_positive, _ = roc_curve(y_test, y_predicted)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(false_positive, true_positive, color='darkorange', label='Random Forest')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve (area = %0.2f)' % auc)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def Print_Metrics(saved_rf):\n",
    "    print('\\nModel performance on the test data set:')\n",
    "\n",
    "    # print('Train Accuracy.......', accuracy_score(y_train, best_model.predict(X_train)))\n",
    "    # print('Validate Accuracy....', accuracy_score(y_valid, best_model.predict(X_valid)))\n",
    "\n",
    "    y_predict_test  = best_model.predict(X_test)\n",
    "    mse             = metrics.mean_squared_error(y_test, y_predict_test)\n",
    "    logloss_test    = metrics.log_loss(y_test, y_predict_test)\n",
    "    accuracy_test   = metrics.accuracy_score(y_test, y_predict_test)\n",
    "    accuracy_test2  = best_model.score(X_test, y_test)\n",
    "    F1_test         = metrics.f1_score(y_test, y_predict_test)\n",
    "    precision_test  = precision_score(y_test, y_predict_test, average='binary')\n",
    "    precision_test2 = metrics.precision_score(y_test, y_predict_test)\n",
    "    recall_test     = recall_score(y_test, y_predict_test, average='binary')\n",
    "    auc_test        = metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    r2_test         = metrics.r2_score(y_test, y_predict_test)\n",
    "\n",
    "    #test_auc       = h2o.get_model(\"best_rf\").model_performance(test_data=test).auc()\n",
    "    #print('Best model performance based on auc: ', test_auc)\n",
    "    \n",
    "    header = [\"Metric\", \"Test\"]\n",
    "    table  = [\n",
    "            [\"logloss\",   logloss_test],\n",
    "            [\"accuracy\",  accuracy_test],\n",
    "            [\"precision\", precision_test],\n",
    "            [\"F1\",        F1_test],\n",
    "            [\"r2\",        r2_test],\n",
    "            [\"AUC\",       auc_test]\n",
    "            ]\n",
    "\n",
    "    print(tabulate(table, header, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "def Plot_predictor_importance(best_model, feature_columns):\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    y_pos  = np.arange(sorted_idx.shape[0]) + .5\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(y_pos, \n",
    "            feature_importance[sorted_idx], \n",
    "            align='center', \n",
    "            color='green', \n",
    "            ecolor='black', \n",
    "            height=0.5)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feature_columns)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Relative Importance')\n",
    "    ax.set_title('Predictor Importance')\n",
    "    plt.show()\n",
    "\n",
    "def Report_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "def Print_confusion_matrix(cm, auc, heading):\n",
    "    print('\\n', heading)\n",
    "    print(cm)\n",
    "    true_negative  = cm[0,0]\n",
    "    true_positive  = cm[1,1]\n",
    "    false_negative = cm[1,0]\n",
    "    false_positive = cm[0,1]\n",
    "    total = true_negative + true_positive + false_negative + false_positive\n",
    "    accuracy = (true_positive + true_negative)/total\n",
    "    precision = (true_positive)/(true_positive + false_positive)\n",
    "    recall = (true_positive)/(true_positive + false_negative)\n",
    "    misclassification_rate = (false_positive + false_negative)/total\n",
    "    F1 = (2*true_positive)/(2*true_positive + false_positive + false_negative)\n",
    "    print('accuracy.................%7.4f' % accuracy)\n",
    "    print('precision................%7.4f' % precision)\n",
    "    print('recall...................%7.4f' % recall)\n",
    "    print('F1.......................%7.4f' % F1)\n",
    "    print('auc......................%7.4f' % auc)\n",
    "\n",
    "def Plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n",
    "                        n_jobs = 1, train_sizes = np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, y,\n",
    "                                                            cv = cv,\n",
    "                                                            n_jobs = n_jobs,\n",
    "                                                            train_sizes = train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def Random_Search():\n",
    "    global best_model, saved_moldel\n",
    "    param_grid = {\"n_estimators\": range(20, 100, 2),\n",
    "                  \"max_depth\": range(4, 50, 2),\n",
    "                  \"min_samples_leaf\": range(2, 100, 2),\n",
    "                  \"max_features\": sp_randint(1, n_features),\n",
    "                  \"min_samples_split\": sp_randint(2, 10),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "    clf = RandomForestClassifier(class_weight = 'balanced')\n",
    "    n_iter_search = 500\n",
    "    estimator = RandomizedSearchCV(clf,\n",
    "                                   param_distributions = param_grid,\n",
    "                                   n_iter = n_iter_search,\n",
    "                                   scoring = 'roc_auc',\n",
    "                                   verbose = 0,\n",
    "                                   n_jobs = 1)\n",
    "        \n",
    "    fit = estimator.fit(X_train, y_train)\n",
    "\n",
    "    # Cross validation with 20 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv_ = ShuffleSplit(n_splits = 20, test_size = 0.20, random_state = 0)\n",
    "    Plot_learning_curve(estimator, \n",
    "                        'Learning Curves',\n",
    "                        X_train, y_train, \n",
    "                        cv = cv_,\n",
    "                        n_jobs = 1)\n",
    "     \n",
    "    Report_scores(estimator.cv_results_, n_top = 3)   \n",
    "    best_model = estimator.best_estimator_\n",
    "    print('\\nbest_model:\\n', best_model)\n",
    "\n",
    "    print('\\nFeature Importances:', best_model.feature_importances_)\n",
    "    Plot_predictor_importance(best_model, feature_columns)\n",
    "\n",
    "    y_predicted = best_model.predict(X_train)\n",
    "    probabilities = best_model.predict_proba(X_train)\n",
    "\n",
    "    c_report = classification_report(y_train, y_predicted)\n",
    "    print('\\nClassification report:\\n', c_report)\n",
    "\n",
    "    y_predicted_train = best_model.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_predicted_train)\n",
    "    auc = roc_auc_score(y_train, y_predicted_train)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the training dataset')\n",
    "\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    auc = roc_auc_score(y_test, y_predicted)\n",
    "\n",
    "    ntotal = len(y_test)\n",
    "    correct = y_test == y_predicted\n",
    "    numCorrect = sum(correct)\n",
    "    percent = round( (100.0*numCorrect)/ntotal, 6)\n",
    "    print(\"\\nCorrect classifications on test data: {0:d}/{1:d} {2:8.3f}%\".format(numCorrect, ntotal, percent))\n",
    "    prediction_score = 100.0*best_model.score(X_test, y_test)\n",
    "    print('Random Forest Prediction Score on test data: %8.3f' % prediction_score)\n",
    "\n",
    "    model_path = 'C:/sm/BottleRockets/trained_models/sklearn_rf_classify.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "\n",
    "    saved_model = joblib.load(model_path)\n",
    "    y_predicted_test = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted_test)\n",
    "    auc = roc_auc_score(y_test, y_predicted_test)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the test dataset')\n",
    "    ROC_Curve(best_model, auc)\n",
    "    Print_Metrics(saved_model)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dataset:\n",
      "     id  email  gender  age  click_rate  geo  device  click\n",
      "0   40      1       2   86    0.147128    1       3      1\n",
      "1   49      2       1   32    0.094582    3       1      0\n",
      "2   89      2       1   38    0.202300    5       2      1\n",
      "3  262      4       1   40    0.458359    6       1      0\n",
      "4  277      1       1   47    0.084073    2       1      0\n",
      "\n",
      "Description of model dataset:\n",
      "                  id        email       gender         age   click_rate  \\\n",
      "count   1000.000000  1000.000000  1000.000000  1000.00000  1000.000000   \n",
      "mean   25258.573000     2.488000     1.376000    52.87500     0.063525   \n",
      "std    14451.237674     1.742668     0.484622    12.26131     0.357769   \n",
      "min       40.000000     1.000000     1.000000    19.00000    -1.000000   \n",
      "25%    12815.500000     1.000000     1.000000    44.00000     0.027751   \n",
      "50%    25049.500000     2.000000     1.000000    52.00000     0.087696   \n",
      "75%    38076.000000     3.000000     2.000000    62.00000     0.174230   \n",
      "max    50806.000000     8.000000     2.000000    98.00000     0.985037   \n",
      "\n",
      "               geo       device  \n",
      "count  1000.000000  1000.000000  \n",
      "mean      2.207000     1.712000  \n",
      "std       1.750204     1.156024  \n",
      "min      -1.000000    -1.000000  \n",
      "25%       1.000000     1.000000  \n",
      "50%       2.000000     2.000000  \n",
      "75%       3.000000     2.000000  \n",
      "max       6.000000     4.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimo/anaconda3/lib/python3.6/site-packages/seaborn/axisgrid.py:2065: UserWarning: The `size` parameter has been renamed to `height`; pleaes update your code.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/massimo/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_timer = Timer()\n",
    "LoadData()\n",
    "Random_Search()\n",
    "elapsed = my_timer.get_time()\n",
    "print(\"\\nTotal compute time was: %s\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
